# -*- coding: utf-8 -*-
"""Speech_to_Text APP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WtqIByhJna9i8VoVpDxvi1liUD32iX5Z

Step 1. Install the Necessary Packages
"""

# Install the necessary packages
!pip uninstall transformers && pip install transformers --upgrade-strategy eager
!pip install sentencepiece
!pip install torch_sdpa

# Purge the cache to free some memories
!python3 -m pip cache purge

# import the necesary packages
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
import torchaudio
import torch
import soundfile as sf
import os

# Import additional supporting packages for Audio files
import librosa # librosa is a Python package for music and audio processing
import librosa.display
import IPython.display as ipd  # to play the audio signal
import matplotlib.pyplot as plt #plot the audio signal in time domain
import requests
import io
import tempfile

# Let's specify the device based on availability of GPU/CPU, enabling an optimized processing based on the available hardware
processing_device = "cuda:0" if torch.cuda.is_available() else "cpu"

"""# Method 1: Wav2Vec2.0 Model"""

# The major models under Wav2Vec2.0 include:
# wav2vec2_model_name = "facebook/wav2vec2-base-960h" # 360MB
# wav2vec2_model_name = "facebook/wav2vec2-large-960h-lv60-self" # pretrained 1.26GB
wav2vec2_model_name = "jonatasgrosman/wav2vec2-large-xlsr-53-english" # English-only, 1.26GB
# wav2vec2_model_name = "jonatasgrosman/wav2vec2-large-xlsr-53-arabic" # Arabic-only, 1.26GB
# wav2vec2_model_name = "jonatasgrosman/wav2vec2-large-xlsr-53-spanish" # Spanish-only, 1.26GB


# Select the English version only since its an English based STT
wav2vec2_processor = Wav2Vec2Processor.from_pretrained(wav2vec2_model_name)
wav2vec2_model = Wav2Vec2ForCTC.from_pretrained(wav2vec2_model_name).to(processing_device)

# audio_url = "http://www.fit.vutbr.cz/~motlicek/sympatex/f2bjrop1.0.wav"
audio_url = "http://www.fit.vutbr.cz/~motlicek/sympatex/f2bjrop1.0.wav"


# Download the audio file
response = requests.get(audio_url)

# Create a temporary file and write the audio data
with tempfile.NamedTemporaryFile(delete=False) as temp_file:
    temp_file.write(response.content)
    temp_file.seek(0)

    # Load the audio file using librosa
    x, sr = librosa.load(temp_file.name)

# Play the audio using IPython display
ipd.Audio(x, rate=sr)

# Let's calculate the time domain analaysis of the audio file

plt.figure(figsize=(14, 5)) # this is the variable to change the rnage on the x and y axis
plt.grid(True) # now the grid is enabled

#  Let's plot the audio signal
librosa.display.waveshow(x) #it takes one argument
plt.xlabel("Time")
plt.ylabel("Amplitude")
plt.title("Time Domain Analysis of the Audio file")

# Let's calculate audio file's duration
y=len(x) #number of sample x
print(y)
y1=sr #sampling frequency sr
print(y1)
y=len(x)
duration_of_sound=y/y1
print(duration_of_sound, "seconds")

# Let's load the audio file
speech, sr = torchaudio.load(audio_url)
# Remove the singleton dimension from the tensor to simplify the shape of the tensor
speech = speech.squeeze()
# Check the shape of the 'speech' tensor
sr, speech.shape

# Resample the audio's sample rate to 16000 Hz
resampler = torchaudio.transforms.Resample(sr, 16000)
speech = resampler(speech)
speech.shape

'''
Tokenize the audio signal using the Wav2Vec2 processor
it returns a dictionary & d "input_values" keys is used to extract the tokenized rep of d audio
'return_tensors="pt"': d output should be in PyTorch tensor fmt
'sampling_rate=16000': specify d sampling rate 4 processing d input audio
'''
input_values = wav2vec2_processor(speech, return_tensors="pt", sampling_rate=16000)["input_values"].to(processing_device)
input_values.shape

'''
d wav2vec2_model is applied to d  input_values tensor.
It returns a dictionary, & d "logits" key is used to extract d model's output logits.
d resulting logits are stored in d logits variable.
'''
logits = wav2vec2_model(input_values)["logits"]
logits.shape

"""- The shape is [1, 556, 33], indicating a 3D tensor.
- The first dimension (1) is the batch size.
- The second dimension (556) represents the sequence length or the number of time steps in the output.
- The third dimension (33) corresponds to the number of classes or features in the output.
"""

'''
- d 'argmax' function is applied to the logits tensor along the last dimension (dim=-1)
- it'll find d index with d max value along d last dimension for each element in d tensor.
- 'predicted_ids': contains d indices of d max values along d last dimension.
'''
# Let's use argmax to find the predicted IDs
predicted_ids = torch.argmax(logits, dim=-1)
predicted_ids.shape

"""- The shape is [1, 556], indicating a 2D tensor:
- The first dimension (1) is the batch size.
- The second dimension (556) represents the sequence length or the number of predicted IDs.
"""

# Now, let's decode the IDs to text
wav2vec2_transcription = wav2vec2_processor.decode(predicted_ids[0])
wav2vec2_transcription.lower()

"""# Method 2: Whisper Models"""

# Install whisper
!pip install --upgrade whisper

# import WhisperProcessor & WhisperForConditionalGeneration
from transformers import WhisperProcessor, WhisperForConditionalGeneration

# Since this is a English based STT project, we chose the English-only medium model
whisper_model_name = "openai/whisper-medium.en" # Engllish-only, ~3.06 GB

whisper_processor = WhisperProcessor.from_pretrained(whisper_model_name)
whisper_model = WhisperForConditionalGeneration.from_pretrained(whisper_model_name).to(processing_device)

'''
- librosa.load: loads d audio data from d content directly. d audio signal x and sample rate sr are obtained.
- whisper_processor: converts d raw audio signal into input features
- d processed features is returned as PyTorch tensors & moved to d processing device
'''
# Use response.content directly without temporary file
x, sr = librosa.load(io.BytesIO(response.content), sr=sr)
input_features = whisper_processor(x, sampling_rate=16000, return_tensors="pt").input_features.to(processing_device)

'''
- Get d decoder prompt IDs from d 'whisper_processor'
- Use d 'get_decoder_prompt_ids' to specify d language (English) & task to perfom (transcribe)
-
'''
forced_decoder_ids = whisper_processor.get_decoder_prompt_ids(language= "english", task="transcribe")

forced_decoder_ids

input_features.shape

predicted_ids = whisper_model.generate(input_features, forced_decoder_ids=forced_decoder_ids)
predicted_ids.shape

whisper_transcription1 = whisper_processor.batch_decode(predicted_ids, skip_special_tokens=True)
whisper_transcription1

whisper_transcription_2 = whisper_processor.batch_decode(predicted_ids, skip_special_tokens=False)
whisper_transcription_2

"""# Part 2: System Performance Calculation  """

# To calculate the wer, we need to use a ground truth that will serve as the original transcript

# Here's the trancription to be used as the ground truth
transcript = "Wanted, Chief Justice of the Massachusetts Supreme Court. In April, the SJC's current leader, Edward Hennessey, reaches the mandatory retirement age of 70, and the successor is"

s# Now let's preprocess our trancription to get it ready for the calculation

import re

def process_text(transcript):
  # Let's convert it to lowercase first
  transcript = transcript.lower()

  # Let's reove special characters and punctuations
  transcript = re.sub(r'[^a-zA-Z0-9\s]','', transcript)

  # let's tokenize the transcription by splitting the texts into words
  tokens = transcript.split()

  return tokens

# Let's check if it was preprocessed successfully
preprocessed_transcript = process_text(transcript)

# print the preprocessed text
print(preprocessed_transcript)

type(preprocessed_transcript)

print(transcript)

print(preprocessed_transcript)

# You might encounter an error with installing "dill" due to the settings made here:
# processing_device = "cuda:0" if torch.cuda.is_available() else "cpu"
# So import these settings

import locale
print(locale.getpreferredencoding())

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

!pip install dill

type(wav2vec2_transcription)

type(whisper_transcription_2)

from transformers import WhisperTokenizer

pip install --upgrade evaluate jiwer

from evaluate import load

print(wav2vec2_transcription)

print(whisper_transcription1)

ref_words = transcript
hyp_words = wav2vec2_transcription

type(whisper_transcription1)

wer_metric = load("wer")

wer = wer_metric.compute(references=[ref_words], predictions=[hyp_words])

print(wer)

# We need to convert the whisper_transaction to str from a list so we can perform the wer
whisper_string = ''.join(whisper_transcription1)
print(whisper_string)

type(whisper_string)

wer_metric = load("wer")

wer = wer_metric.compute(references=[ref_words], predictions=[whisper_string])

print(wer)