# -*- coding: utf-8 -*-
"""Speech_to_Text System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10ykc1bjEzV3GTwx3WwgZXP3n_6RCTpwt

Step 1. Install the Necessary Packages
"""

# @title
# Upload the raw audio file
# uploaded = files.upload()

# @title
# Access the uploaded file
# file_name = next(iter(uploaded))

# ipd.Audio(new_audio, rate=sr)

# IT STARTS HERE

!pip uninstall transformers  -y
!pip install transformers

import soundfile as sf

!pip install --upgrade transformers

!pip install sentencepiece

pip show transformers

pip install transformers --upgrade-strategy eager

pip uninstall transformers && pip install transformers

!python3 -m pip cache purge

!pip install torch_sdpa

!pip install --upgrade transformers

from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
import torchaudio
import torch

# Check and set the device (GPU if available, CPU otherwise)
processing_device = "cuda" if torch.cuda.is_available() else "cpu"

from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC
import torch
import soundfile as sf

# Let's import necessary audio processing libraries
import os
import torchaudio

# Let's specify the device based on availability of GPU/CPU, enabling an optimized processing based on the available hardware
processing_device = "cuda:0" if torch.cuda.is_available() else "cpu"

"""# Method 1: Wav2Vec2.0 Model"""

# The major models under Wav2Vec2.0 include:
# wav2vec2_model_name = "facebook/wav2vec2-base-960h" # 360MB
# wav2vec2_model_name = "facebook/wav2vec2-large-960h-lv60-self" # pretrained 1.26GB
wav2vec2_model_name = "jonatasgrosman/wav2vec2-large-xlsr-53-english" # English-only, 1.26GB
# wav2vec2_model_name = "jonatasgrosman/wav2vec2-large-xlsr-53-arabic" # Arabic-only, 1.26GB
# wav2vec2_model_name = "jonatasgrosman/wav2vec2-large-xlsr-53-spanish" # Spanish-only, 1.26GB


# my project focused on a Enghlish speech to text conversion, hence:
wav2vec2_processor = Wav2Vec2Processor.from_pretrained(wav2vec2_model_name)
wav2vec2_model = Wav2Vec2ForCTC.from_pretrained(wav2vec2_model_name).to(processing_device)

# Supporting Libariries for Audio files
import librosa # librosa is a Python package for music and audio processing
import librosa.display
import IPython.display as ipd  # to play the audio signal
import matplotlib.pyplot as plt #plot the audio signal in time domain

import requests
import librosa
import io
import tempfile

# audio_url = "http://www.fit.vutbr.cz/~motlicek/sympatex/f2bjrop1.0.wav"
audio_url = "http://www.fit.vutbr.cz/~motlicek/sympatex/f2bjrop1.0.wav"


# Download the audio file
response = requests.get(audio_url)

# Create a temporary file and write the audio data
with tempfile.NamedTemporaryFile(delete=False) as temp_file:
    temp_file.write(response.content)
    temp_file.seek(0)

    # Load the audio file using librosa
    x, sr = librosa.load(temp_file.name)

# Play the audio using IPython display
ipd.Audio(x, rate=sr)

# Let's calculate the time domain analaysis of the audio file

plt.figure(figsize=(14, 5)) # this is the variable to change the rnage on the x and y axis
plt.grid(True) # now the grid is enabled

#  Let's plot the audio signal
librosa.display.waveshow(x) #it takes one argument
plt.xlabel("Time")
plt.ylabel("Amplitude")
plt.title("Time Domain Analysis of the Audio file")

# Let's determine the duration of the audio file
y=len(x) #number of sample x
print(y)
y1=sr #sampling frequency sr
print(y1)
y=len(x)
duration_of_sound=y/y1
print(duration_of_sound, "seconds")

# Let's load the audio file
speech, sr = torchaudio.load(audio_url)
speech = speech.squeeze()
sr, speech.shape

# Now, let's resample the audio sampling form it's initial rate to 1600
resampler = torchaudio.transforms.Resample(sr, 16000)
speech = resampler(speech)
speech.shape

# Now, let's tokenize the wav file
input_values = wav2vec2_processor(speech, return_tensors="pt", sampling_rate=16000)["input_values"].to(processing_device)
input_values.shape

# Next, let's perform inference
logits = wav2vec2_model(input_values)["logits"]
logits.shape

# Let's use argmax to find the predicted IDs
predicted_ids = torch.argmax(logits, dim=-1)
predicted_ids.shape

# Now, let's decode the IDs to text
wav2vec2_transcription = wav2vec2_processor.decode(predicted_ids[0])
wav2vec2_transcription.lower()

"""# Method 2: Whisper Models"""

!pip install --upgrade whisper

from transformers import WhisperProcessor, WhisperForConditionalGeneration

# Since this is a English based STT project, we chose the English-only medium model
whisper_model_name = "openai/whisper-medium.en" # Engllish-only, ~3.06 GB

whisper_processor = WhisperProcessor.from_pretrained(whisper_model_name)
whisper_model = WhisperForConditionalGeneration.from_pretrained(whisper_model_name).to(processing_device)

# Use response.content directly without temporary file
x, sr = librosa.load(io.BytesIO(response.content), sr=sr)
input_features = whisper_processor(x, sampling_rate=16000, return_tensors="pt").input_features.to(processing_device)

forced_decoder_ids = whisper_processor.get_decoder_prompt_ids(language= "english", task="transcribe")

forced_decoder_ids

input_features.shape

predicted_ids = whisper_model.generate(input_features, forced_decoder_ids=forced_decoder_ids)
predicted_ids.shape

whisper_transcription1 = whisper_processor.batch_decode(predicted_ids, skip_special_tokens=True)
whisper_transcription1

whisper_transcription_2 = whisper_processor.batch_decode(predicted_ids, skip_special_tokens=False)
whisper_transcription_2

"""# Part 2: System Performance Calculation  """

# To calculate the wer, we need to use a ground truth that will serve as the original transcript

# Here's the trancription to be used as the ground transcription
transcript = "Wanted, Chief Justice of the Massachusetts Supreme Court. In April, the SJC's current leader, Edward Hennessey, reaches the mandatory retirement age of 70, and the successor is"

# Now let's preprocess our trancription to get it ready for the calculation

import re

def process_text(transcript):
  # Let's convert it to lowercase first
  transcript = transcript.lower()

  # Let's reove special characters and punctuations
  transcript = re.sub(r'[^a-zA-Z0-9\s]','', transcript)

  # let's tokenize the transcription by splitting the texts into words
  tokens = transcript.split()

  return tokens

# Let's check if it was preprocessed successfully
preprocessed_transcript = process_text(transcript)

# print the preprocessed text
print(preprocessed_transcript)

type(preprocessed_transcript)

print(transcript)

print(preprocessed_transcript)

pip install dill

type(wav2vec2_transcription)

type(whisper_transcription_2)

from transformers import WhisperTokenizer

pip install --upgrade evaluate jiwer

from evaluate import load

print(wav2vec2_transcription)

print(whisper_transcription1)

ref_words = transcript
hyp_words = wav2vec2_transcription

type(whisper_transcription1)

wer_metric = load("wer")

wer = wer_metric.compute(references=[ref_words], predictions=[hyp_words])

print(wer)

# We need to convert the whisper_transaction to str from a list so we can perform the wer
whisper_string = ''.join(whisper_transcription1)
print(whisper_string)

type(whisper_string)

wer_metric = load("wer")

wer = wer_metric.compute(references=[ref_words], predictions=[whisper_string])

print(wer)

!pip install pydub

"""# Section 3: Raw record"""

from pydub import AudioSegment

from google.colab import files

from google.colab import files
from pydub import AudioSegment
from io import BytesIO

# Upload the raw audio file
uploaded = files.upload()

# Access the uploaded file
file_name = next(iter(uploaded))

# Convert the uploaded audio file to AudioSegment
audio_data = BytesIO(uploaded[file_name])
audio_record = AudioSegment.from_file(audio_data, format="aac")

# Export the audio to WAV format
new_audio = 'new_audio.wav'
audio_record.export(new_audio, format="wav")

print(f"Conversion completed. WAV file saved as {new_audio}")

ipd.Audio(new_audio, rate=sr)

audio_data = open('new_audio.wav', 'rb').read()

# Let's determine the duration of the audio file
y=len(audio_data) #number of sample x
print(y)
y1=sr #sampling frequency sr
print(y1)
y=len(audio_data)
duration_of_sound=y/y1
print(duration_of_sound, "seconds")

# Step 1: Load the audio file
import torchaudio
from torchaudio.transforms import Resample

# Load the audio file
audio_path = 'new_audio.wav'
speech, sr = torchaudio.load(audio_path)
speech = speech.squeeze()

# Step 2: Resample the audio to 16,000 Hz
resampler = Resample(sr, 16000)
speech_resampled = resampler(speech)

# Step 3: Prepare the audio for transcription with Whisper model
# Assuming 'whisper_processor' and 'processing_device' are initialized

# Tokenize the resampled audio
input_values = whisper_processor(speech_resampled, sampling_rate=16000, return_tensors="pt").input_features.to(processing_device)

# Step 4: Perform inference (transcription) with the Whisper model
# Assuming 'whisper_model' is initialized
logits = whisper_model(input_values)["logits"]
# Process logits further if necessary to get the transcription result



# import speech_recognition as sr

# recognizer = sr.Recognizer()

''' recording the sound '''

# with sr.AudioFile("./sample_audio/speech.wav") as source:
    #recorded_audio = recognizer.listen(source)
    #print("Done recording")

''' Recorgnizing the Audio '''
#try:
    #print("Recognizing the text")
    #text = recognizer.recognize_google(
            #recorded_audio,
            #language="en-US"
        #)
    #print("Decoded Text : {}".format(text))

#except Exception as ex:
    #print(ex)