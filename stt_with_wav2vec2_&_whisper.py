# -*- coding: utf-8 -*-
"""STT with Wav2Vec2 & Whisper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WtqIByhJna9i8VoVpDxvi1liUD32iX5Z

Step 1. Install the Necessary Packages
"""

# Install the necessary packages
!pip install transformers --upgrade-strategy eager
!pip install sentencepiece
!pip install torch_sdpa

# Purge the cache to free some memories
!python3 -m pip cache purge

# import the necesary packages
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
import torchaudio
import torch
import soundfile as sf
import os

# Import additional supporting packages for Audio files
import librosa # librosa is a Python package for music and audio processing
import librosa.display
import IPython.display as ipd  # to play the audio signal
import matplotlib.pyplot as plt #plot the audio signal in time domain
import requests
import io
import tempfile

# Let's specify the device based on availability of GPU/CPU, enabling an optimized processing based on the available hardware
processing_device = "cuda:0" if torch.cuda.is_available() else "cpu"

"""# Method 1: Wav2Vec2.0 Model"""

# The major models under Wav2Vec2.0 include:
# wav2vec2_model_name = "facebook/wav2vec2-base-960h" # 360MB
# wav2vec2_model_name = "facebook/wav2vec2-large-960h-lv60-self" # pretrained 1.26GB
wav2vec2_model_name = "jonatasgrosman/wav2vec2-large-xlsr-53-english" # English-only, 1.26GB
# wav2vec2_model_name = "jonatasgrosman/wav2vec2-large-xlsr-53-arabic" # Arabic-only, 1.26GB
# wav2vec2_model_name = "jonatasgrosman/wav2vec2-large-xlsr-53-spanish" # Spanish-only, 1.26GB


# Select the English version only since its an English based STT
wav2vec2_processor = Wav2Vec2Processor.from_pretrained(wav2vec2_model_name)
wav2vec2_model = Wav2Vec2ForCTC.from_pretrained(wav2vec2_model_name).to(processing_device)

audio_url = "http://www.fit.vutbr.cz/~motlicek/sympatex/f2bjrop1.0.wav"

# Download the audio file
response = requests.get(audio_url)

# Create a temporary file and write the audio data
with tempfile.NamedTemporaryFile(delete=False) as temp_file:
    temp_file.write(response.content)
    temp_file.seek(0)

    # Load the audio file using librosa
    x, sr = librosa.load(temp_file.name)

# Play the audio using IPython display
ipd.Audio(x, rate=sr)

# Calculate the time domain analaysis of the audio file

plt.figure(figsize=(14, 5)) # this is the variable to change the rnage on the x and y axis
plt.grid(True) # now the grid is enabled

#  Let's plot the audio signal
librosa.display.waveshow(x) #it takes one argument
plt.xlabel("Time")
plt.ylabel("Amplitude")
plt.title("Time Domain Analysis of the Audio file")
plt.show()

# Let's calculate audio file's duration
y=len(x) #number of sample x
print(y)
y1=sr #sampling frequency sr
print(y1)
y=len(x)
duration_of_sound=y/y1
print(duration_of_sound, "seconds")

# Let's load the audio file
speech, sr = torchaudio.load(audio_url)
# Remove the singleton dimension from the tensor to simplify the shape of the tensor
speech = speech.squeeze()
# Check the shape of the 'speech' tensor
sr, speech.shape

# Resample the audio's sample rate to 16000 Hz
resampler = torchaudio.transforms.Resample(sr, 16000)
speech = resampler(speech)
speech.shape

'''
Tokenize the audio signal using the Wav2Vec2 processor
it returns a dictionary & d "input_values" keys is used to extract the tokenized rep of d audio
'return_tensors="pt"': d output should be in PyTorch tensor fmt
'sampling_rate=16000': specify d sampling rate 4 processing d input audio
'''
input_values = wav2vec2_processor(speech, return_tensors="pt", sampling_rate=16000)["input_values"].to(processing_device)
input_values.shape

'''
d wav2vec2_model is applied to d  input_values tensor.
It returns a dictionary, & d "logits" key is used to extract d model's output logits.
d resulting logits are stored in d logits variable.
'''
logits = wav2vec2_model(input_values)["logits"]
logits.shape

"""- The shape is [1, 556, 33], indicating a 3D tensor.
- The first dimension (1) is the batch size.
- The second dimension (556) represents the sequence length or the number of time steps in the output.
- The third dimension (33) corresponds to the number of classes or features in the output.
"""

'''
- d 'argmax' function is applied to the logits tensor along the last dimension (dim=-1)
- it'll find d index with d max value along d last dimension for each element in d tensor.
- 'predicted_ids': contains d indices of d max values along d last dimension.
'''
# Let's use argmax to find the predicted IDs
predicted_ids = torch.argmax(logits, dim=-1)
predicted_ids.shape

"""- The shape is [1, 556], indicating a 2D tensor:
- The first dimension (1) is the batch size.
- The second dimension (556) represents the sequence length or the number of predicted IDs.
"""

# Now, let's decode the IDs to text
wav2vec2_transcription = wav2vec2_processor.decode(predicted_ids[0])
wav2vec2_transcription.lower()

"""# Method 2: Whisper Models"""

# Install whisper
!pip install --upgrade whisper

# import WhisperProcessor & WhisperForConditionalGeneration
from transformers import WhisperProcessor, WhisperForConditionalGeneration

# Since this is a English based STT project, we chose the English-only medium model
whisper_model_name = "openai/whisper-medium.en" # Engllish-only, ~3.06 GB

whisper_processor = WhisperProcessor.from_pretrained(whisper_model_name)
whisper_model = WhisperForConditionalGeneration.from_pretrained(whisper_model_name).to(processing_device)

'''
- librosa.load: loads d audio data from d content directly. d audio signal x and sample rate sr are obtained.
- whisper_processor: converts d raw audio signal into input features
- d processed features is returned as PyTorch tensors & moved to d processing device
'''
# Use response.content directly without temporary file
x, sr = librosa.load(io.BytesIO(response.content), sr=sr)
input_features = whisper_processor(x, sampling_rate=16000, return_tensors="pt").input_features.to(processing_device)

# Generate sequence predictions using whisper_model with input_features and forced_decoder_ids
forced_decoder_ids = whisper_processor.get_decoder_prompt_ids(language= "english", task="transcribe")

# List of tuples representing forced decoder prompt IDs for guiding model generation.
forced_decoder_ids

# Print the shape of the input_features
input_features.shape

# Generate sequence predictions using whisper_model with input_features and forced_decoder_ids
predicted_ids = whisper_model.generate(input_features, forced_decoder_ids=forced_decoder_ids)
predicted_ids.shape

# Decode predicted_ids using whisper_processor, excluding special tokens
# The result is stored in whisper_transcription1.
whisper_transcription1 = whisper_processor.batch_decode(predicted_ids, skip_special_tokens=True)
whisper_transcription1

# Decode predicted_ids using whisper_processor, including special tokens
# The result is stored in whisper_transcription_2.
whisper_transcription_2 = whisper_processor.batch_decode(predicted_ids, skip_special_tokens=False)
whisper_transcription_2

"""# Part 2: System Performance Calculation  """

# To calculate the Word Error Rate (WER), we need a ground truth that represents the correct transcription
transcript = "Wanted, Chief Justice of the Massachusetts Supreme Court. In April, the SJC's current leader, Edward Hennessey, reaches the mandatory retirement age of 70, and the successor is"

# Preprocess the trancription to get it ready for the calculation

import re

def process_text(transcript):
  # Let's convert the transcript to lowercase
  transcript = transcript.lower()

  # Remove special characters and punctuations
  transcript = re.sub(r'[^a-zA-Z0-9\s]','', transcript)

  # Tokenize the transcript by splitting the texts into words
  tokens = transcript.split()

  return tokens

# Check if it was preprocessed successfully
preprocessed_transcript = process_text(transcript)

# print the preprocessed text
print(preprocessed_transcript)

# Check the type of the variable preprocessed_transcript.
type(preprocessed_transcript)

print(transcript)

print(preprocessed_transcript)

# You might encounter an error with installing "dill" due to this configuration from earlier cells:
# processing_device = "cuda:0" if torch.cuda.is_available() else "cpu"

# So import these settings
import locale
print(locale.getpreferredencoding())

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

!pip install dill

# Check the type of the variable wav2vec2_transcription, whcih represents the wav2vec
type(wav2vec2_transcription)

# Check the type of the whisper_transcription
type(whisper_transcription_2)

# import WhisperTokenizer
from transformers import WhisperTokenizer

# Install and upgrade Jiwer & Evaluate, 2 important packages for calculating WER
!pip install --upgrade evaluate jiwer

# Import load from
from evaluate import load

# Check the wav2vec2 transcription again beofre calculating the WER
print(wav2vec2_transcription)

# Also check the whisper_transcription
print(whisper_transcription1)

# assign the ground truth (transcript) to ref_words
ref_words = transcript

# Assign the wav2vec2 transcription to hyp_words
hyp_words = wav2vec2_transcription

# Load the Word Error Rate metric from the evaluate library
wer_metric = load("wer")

# references: contains the reference (ground truth) sequence(s).
# predictions: contains the predicted sequnce(s)
wav2vec2_wer = wer_metric.compute(references=[ref_words], predictions=[hyp_words])

print(wav2vec2_wer)

# Check the type of whisper_transcription
type(whisper_transcription1)

# Print whisper_transcription1 to confirm the type
print(whisper_transcription1)

# We need to convert the whisper_transaction from a list to a str we can perform the wer
whisper_string = ' '.join(whisper_transcription1)
print(whisper_string)

type(whisper_string)

# Load the Word Error Rate metric from the evaluate library
wer_metric = load("wer")

# references: contains the reference (ground truth) sequence(s).
# predictions: contains the predicted sequnce(s)
whisper_wer = wer_metric.compute(references=[ref_words], predictions=[whisper_string])

print(whisper_wer)

